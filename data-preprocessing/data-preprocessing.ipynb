{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66a9fdd",
   "metadata": {},
   "source": [
    "## Data Preprocessing in Machine Learning ⭐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9796a0",
   "metadata": {},
   "source": [
    "The goal is simple: **prepare raw data so our model can learn patterns instead of noise**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38fb7f",
   "metadata": {},
   "source": [
    "**The Complete  Data Preprocessing Pipeline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069e597",
   "metadata": {},
   "source": [
    "1️⃣ **Data Collection**\n",
    "\n",
    "You gather raw data from:\n",
    "\n",
    "- CSV files\n",
    "- Databases\n",
    "- APIs\n",
    "- Web scraping\n",
    "- Sensors, logs\n",
    "- Data warehouses (Snowflake, BigQuery, etc.)\n",
    "\n",
    "No preprocessing yet — just collecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6d7af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sleep_gym.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a155c96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  hours_sleep  energy_level  went_gym\n",
      "0        1          7.5             8         1\n",
      "1        2          5.0             4         0\n",
      "2        3          6.0             6         1\n",
      "3        4          4.5             3         0\n",
      "4        5          8.0             9         1\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ec7e3",
   "metadata": {},
   "source": [
    "2️⃣ **Data Understanding / Exploration (EDA = Exploratory Data Analysis)**\n",
    "\n",
    "Before cleaning anything, you look at (analyze) your data to understand it.\n",
    "\n",
    "You check:\n",
    "\n",
    "- What each column means\n",
    "- Which columns are numeric, categorical, text, dates\n",
    "- Shape of dataset\n",
    "- Missing values\n",
    "- Outliers\n",
    "- Distribution of each feature\n",
    "- Correlations\n",
    "\n",
    "Tools:\n",
    "\n",
    "- df.info(), df.describe()\n",
    "- Histograms\n",
    "- Boxplots\n",
    "- Pairplots\n",
    "- Correlation matrix\n",
    "\n",
    "This helps you decide what cleaning/transformation is needed.\n",
    "\n",
    "--\n",
    "\n",
    "**❗IMPORTANT:**\n",
    "**EDA — Understanding the Dataset’s Nature and Context**\n",
    "\n",
    "Before performing any cleaning, scaling, or transformation, **it is crucial to understand the nature of the dataset** you’re working with. EDA is not just about generating plots and summaries — **it’s about understanding the context in which the data was collected** and **what each feature truly represents**.\n",
    "\n",
    "Many preprocessing steps (like dropping columns, filling missing values, removing outliers, or smoothing noisy data) can accidentally delete **meaningful information** if you don’t understand this context.\n",
    "\n",
    "For example:\n",
    "\n",
    "- A “missing” value might actually mean the event didn’t happen (not an error).\n",
    "- A “weird” value might be rare but important (fraud detection, medical anomalies).\n",
    "- An outlier might represent a critical edge case rather than noise.\n",
    "- A categorical label with only a few samples might be a rare but valid class.\n",
    "\n",
    "Therefore, a major part of EDA is to make sure you understand:\n",
    "\n",
    "- How the data was collected\n",
    "- What each feature means\n",
    "- Why some values may be missing or extreme\n",
    "- Whether irregularities are errors or meaningful signals\n",
    "\n",
    "Only after understanding the story behind the data can you decide the right preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeed183",
   "metadata": {},
   "source": [
    "3️⃣ **Data Cleaning**\n",
    "\n",
    "This is the biggest and most important part.\n",
    "\n",
    "Includes:\n",
    "\n",
    "**✔ Handling Missing Values**\n",
    "\n",
    "Ways:\n",
    "\n",
    "- Delete rows/columns (if too many missing)\n",
    "- Fill with mean/median (numeric)\n",
    "- Fill with mode (categorical)\n",
    "- Forward/backward fill (time series)\n",
    "- Use models to predict missing values\n",
    "\n",
    "**✔ Handling Noise / Errors**\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Wrong values (“age = 300”)\n",
    "- Duplicates\n",
    "- Typos (“Male”, “male”, “MALE”)\n",
    "\n",
    "**✔ Outlier Detection**\n",
    "\n",
    "Methods:\n",
    "\n",
    "- Z-Score\n",
    "- IQR\n",
    "- Isolation Forest\n",
    "- Boxplot inspection\n",
    "\n",
    "You either remove, cap, or transform outliers.\n",
    "\n",
    "**✔ Data Type Corrections**\n",
    "\n",
    "- Converting strings → dates\n",
    "- Strings → categories\n",
    "- Float → int\n",
    "- Object type → numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f848e1c5",
   "metadata": {},
   "source": [
    "4️⃣ **Data Integration (if you have multiple data sources)**\n",
    "\n",
    "When data comes from different places, you must integrate it.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- Merge tables (SQL joins)\n",
    "- Resolve inconsistencies across systems\n",
    "    - Different names (\"user_id\" vs \"uid\")\n",
    "    - Different units (kg vs lbs)\n",
    "    - Different formats (timestamps)\n",
    "- Detect duplicate records across datasets\n",
    "\n",
    "Example:\n",
    "\n",
    "User info in MySQL + user activity in MongoDB → must combine into a single dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17d93f",
   "metadata": {},
   "source": [
    "5️⃣ **Data Transformation**\n",
    "\n",
    "This prepares the data for the algorithm.\n",
    "\n",
    "**✔ Scaling / Normalization**\n",
    "\n",
    "Used for algorithms sensitive to distance:\n",
    "\n",
    "- KNN\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Neural Networks\n",
    "- K-means\n",
    "\n",
    "Methods:\n",
    "\n",
    "- Standardization\n",
    "- Min-Max scaling\n",
    "\n",
    "\n",
    "**✔ Encoding Categorical Variables**\n",
    "\n",
    "For ML:\n",
    "\n",
    "- One-hot encoding (small categories)\n",
    "- Label encoding (ordinal data)\n",
    "- Target encoding (for large cardinality)\n",
    "\n",
    "**✔ Binning**\n",
    "\n",
    "- Converting numeric data to categories (e.g. Age → Age groups).\n",
    "\n",
    "**✔ Feature Construction**\n",
    "\n",
    "Creating new important variables:\n",
    "\n",
    "- BMI = weight / height²\n",
    "- Day of week from date\n",
    "- Length of text input\n",
    "- Interaction terms (feature1 × feature2)\n",
    "\n",
    "**✔ Text preprocessing**\n",
    "\n",
    "- Tokenization\n",
    "- Lowercasing\n",
    "- Stopword removal\n",
    "- Lemmatization\n",
    "- TF-IDF\n",
    "- Embeddings (Word2Vec, BERT)\n",
    "\n",
    "**✔ Time-series preprocessing**\n",
    "\n",
    "- Creating lags\n",
    "- Rolling means\n",
    "- Seasonality decomposition\n",
    "- Differencing (for stationarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be1305",
   "metadata": {},
   "source": [
    "6️⃣ **Data Reduction (optional but useful)**\n",
    "\n",
    "This step makes data smaller, faster, and cleaner.\n",
    "\n",
    "**✔ Dimensionality Reductio**\n",
    "\n",
    "- PCA\n",
    "- t-SNE\n",
    "- UMAP\n",
    "\n",
    "**✔ Feature Selection**\n",
    "\n",
    "- Filter methods (correlation, chi-square)\n",
    "- Wrapper methods (RFE)\n",
    "- Embedded methods (Lasso)\n",
    "\n",
    "**✔ Sampling**\n",
    "\n",
    "- Downsampling\n",
    "- Upsampling (SMOTE for classification)\n",
    "\n",
    "Useful when dataset is extremely large or imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547bb3b",
   "metadata": {},
   "source": [
    "7️⃣ **Train-Test Split**\n",
    "\n",
    "You must separate the data to evaluate fairly.\n",
    "\n",
    "Typical:\n",
    "\n",
    "- Train: 70–80%\n",
    "- Test: 20–30%\n",
    "\n",
    "Sometimes also:\n",
    "\n",
    "- Validation set\n",
    "- K-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eea681",
   "metadata": {},
   "source": [
    "8️⃣ **Final Preprocessing Pipeline (for production)**\n",
    "\n",
    "You package everything into a data pipeline (sklearn Pipeline):\n",
    "\n",
    "✔ Scaling\n",
    "✔ Encoding\n",
    "✔ Imputation\n",
    "✔ Feature selection\n",
    "✔ Model\n",
    "\n",
    "This ensures:\n",
    "\n",
    "- No data leakage\n",
    "- Reproducibility\n",
    "- Cleaner code\n",
    "- Same transformation in training & prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
